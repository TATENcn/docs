---
name: 服务器集群使用说明
description: TATEN服务器集群使用说明，包括集群架构、部署流程、维护指南等内容，帮助用户了解和使用TATEN服务器集群。
---

## 一、服务器集群架构说明

### 1.集群的创建

2026年2月14日,团队成员CN059购买了一台8核心8GB的香港VPS服务器,并将其作为TATEN的云服务器使用.随后几天,CN059又购买了数台美国的高速VPS,并开始了TATEN云计算服务器集群的搭建进程.

集群采用了[Monitor探针监控系统](https://github.com/komari-monitor/komari)来对整个集群的服务器实时性能进行监控,可以通过监控系统的[仪表盘](https://status.powercess.com)查看每台服务器的CPU、内存、网络等性能指标,以便对服务器的运行状态进行统一的查看和管理.

2026年2月19-21日,XinHallow和CN059在香港一服务器上部署了kubernetes的master节点,并将美国一、美国二、美国三共三台服务器作为worker节点加入了kubernetes集群中.

随后购置了美国四节点,我们将美国四节点作为第二个master节点加入集群中.

### 2.集群的架构

集群共有五台服务器,分别是一台香港服务器和四台美国服务器.香港一服务器和美国四服务器是控制平面的master节点,美国一服务器是整个集群的nfs服务器,负责集群的持久化存储.通过 **PersistentVolume（PV）** 和 **PersistentVolumeClaim（PVC）** 机制将 NFS 存储抽象为 Kubernetes 资源

### 3.集群的详细组件

集群的CNI采用[cilium](https://cilium.io/),Ingress采用cilium的gateway功能,使用coreDNS作为DNS服务器,使用动态PVC作为持久化存储方案.

## 二、如何使用集群部署自己的服务

### 1.准备连接配置文件

你需要下载kubectl工具，以实现集群的连接.你可以查看[kubectl安装指南](https://kubernetes.io/zh-cn/docs/tasks/tools/)来获取安装kubectl的详细步骤.

设置好后,将集群连接配置的config文件放置在默认位置（~/.kube/config）,集群的config连接配置文件可以向集群管理员申请获取.

### 2.验证连接

打开终端，输入以下命令来验证你是否成功连接到了集群：

```bash
kubectl get nodes
```

出现这样的输出代表连接成功：

```log
cn059@cn059-2 Projects % kubectl get nodes
NAME STATUS ROLES AGE VERSION
hd-server Ready worker 4h26m v1.35.1
hka-k8s-server Ready control-plane 47h v1.35.1
lhka-k8s-worker Ready worker 98m v1.35.1
usa-k8s-worker Ready storage 43h v1.35.1
usc-k8s-worker Ready worker 46h v1.35.1
usd-k8s-master Ready control-plane 33h v1.35.1
use-k8s-worker Ready database 12h v1.35.1
```

### 3.编写自己的服务的配置文件

我们将采用更加完善的集群管理模式,做到一切配置都可追溯,不过目前服务器还没有部署CI/CD流水线,所以目前只能通过手动部署的方式来完成服务部署.

不过要注意的是,我们的yaml配置文件都需要写好,验证后才可以部署.

使用Git从GitHub仓库中拉取[gitops仓库](https://github.com/TATENcn/gitops)，这个仓库是我们的GitOps仓库，里面计划存放所有集群的配置文件，以及CI/CD流水线。截止目前,仍有很多的配置信息没有迁移,我们将尽快完成迁移工作.

::code-preview
---
class: "[&>div]:*:my-0 [&>div]:*:w-full"
---
:::code-tree{default-value="base/astrbot/00-namespace.yaml"}
```yaml [base/astrbot/00-namespace.yaml]
apiVersion: v1
kind: Namespace
metadata:
  name: astrbot
```

```yaml [base/astrbot/01-pvc.yaml]
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: astrbot-data-pvc
  namespace: astrbot
  labels:
    app: astrbot
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
  # 我们已经在集群中部署了一个全局的动态nfs存储方案,所以这里的storageClassName可以直接设置为 standard
  storageClassName: standard # uncomment and set proper StorageClass
```

```yaml [base/astrbot/02-deployment.yaml]
apiVersion: apps/v1
kind: Deployment
metadata:
  name: astrbot
  namespace: astrbot
  labels:
    app: astrbot
spec:
  replicas: 1
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app: astrbot
  template:
    metadata:
      labels:
        app: astrbot
    spec:
      containers:
        - name: astrbot
          image: soulter/astrbot:latest
          imagePullPolicy: IfNotPresent
          env:
            - name: TZ
              value: "Asia/Shanghai"
          ports:
            - containerPort: 6185
              name: webui
            - containerPort: 6199
              name: qq-ws
            # - containerPort: 6195
            #   name: wecom-wh
            # - containerPort: 6196
            #   name: qq-off-wh
          volumeMounts:
            - name: data
              mountPath: /AstrBot/data
            - name: localtime
              mountPath: /etc/localtime
              readOnly: true
      volumes:
        - name: data
          persistentVolumeClaim:
            claimName: astrbot-data-pvc # 因为PVC是全局的，所以直接引用即可
        - name: localtime
          hostPath:
            path: /etc/localtime
            type: File
```

```yaml [base/astrbot/03-service-nodeport.yaml]
apiVersion: v1
kind: Service
metadata:
  name: astrbot
  namespace: astrbot
  labels:
    app: astrbot
spec:
  type: NodePort
  selector:
    app: astrbot
  ports:
    - name: webui
      port: 6185
      targetPort: 6185
      nodePort: 30185
    - name: qq-ws
      port: 6199
      targetPort: 6199
      nodePort: 30199
    # - name: wecom-wh
    #   port: 6195
    #   targetPort: 6195
    #   nodePort: 30195
    # - name: qq-off-wh
    #   port: 6196
    #   targetPort: 6196
    #   nodePort: 30196
```

```yaml [base/napcat/00-namespace.yaml]
apiVersion: v1
kind: Namespace
metadata:
  name: napcat
```

```yaml [base/napcat/01-pvc.yaml]
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: napcat-data-pvc
  namespace: napcat
  labels:
    app: napcat
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 512Mi
  storageClassName: standard # uncomment and set proper StorageClass
```

```yaml [base/napcat/02-deployment.yaml]
apiVersion: apps/v1
kind: Deployment
metadata:
  name: napcat
  namespace: napcat
  labels:
    app: napcat
spec:
  replicas: 1
  selector:
    matchLabels:
      app: napcat
  template:
    metadata:
      labels:
        app: napcat
    spec:
      containers:
        - name: napcat
          image: mlikiowa/napcat-docker:latest
          imagePullPolicy: Always
          ports:
            - containerPort: 3000
            - containerPort: 3001
            - containerPort: 6099
          volumeMounts:
            - name: napcat-storage
              mountPath: /app/.config/QQ
            - name: napcat-storage
              mountPath: /app/napcat/config
            - name: napcat-storage
              mountPath: /app/napcat/plugins
      volumes:
        - name: napcat-storage
          persistentVolumeClaim:
            claimName: napcat-data-pvc # 引用全局动态 PVC
```

```yaml [base/napcat/03-service.yaml]
apiVersion: v1
kind: Service
metadata:
  name: napcat-webui
  namespace: napcat
spec:
  selector:
    app: napcat # 必须匹配 Pod 的 labels
  ports:
    - name: http
      port: 6099
      targetPort: 6099
  type: ClusterIP
```

```yaml [base/napcat/04-gateway.yaml]
# napcat-gateway.yaml
apiVersion: gateway.networking.k8s.io/v1
kind: Gateway
metadata:
  name: napcat-gateway
  namespace: napcat
spec:
  gatewayClassName: cilium # 使用 Cilium 的 GatewayClass
  listeners:
    - name: http
      port: 80
      protocol: HTTP
      # 允许所有 host（或指定域名）
      allowedRoutes:
        namespaces:
          from: Same # 只允许同 namespace 的路由绑定
```

```yaml [base/napcat/05-router.yaml]
# napcat-route.yaml
apiVersion: gateway.networking.k8s.io/v1
kind: HTTPRoute
metadata:
  name: napcat-webui-route
  namespace: napcat
spec:
  parentRefs:
    - name: napcat-gateway
  hostnames:
    - "napcat.taten.org" # 只响应该域名的请求
  rules:
    - matches:
        - path:
            type: PathPrefix
            value: / # 根路径即可
      backendRefs:
        - name: napcat-webui
          port: 6099
```
:::
::

如上述配置所示，我们创建了两个命名空间部署astrbot和napcat搭建QQ机器人.

你也可以在目录下新建文件夹编写自己的服务的配置文件,在编写配置文件之前,请提前学习kubernetes的[集群架构](https://kubernetes.io/zh-cn/docs/concepts/architecture/)和[配置文件](https://kubernetes.io/zh-cn/docs/tasks/tools/)编写方法

你可以去互联网搜索kubernetes的相关教程进行学习,这里不再赘述.

### 4.部署服务

当你编写好配置文件后,可以使用`kubectl apply`命令来部署服务:

```bash
kubectl apply -f your-service-config.yaml
```

逐个运行命令,并及时查看对应yaml文件的运行结果,提前发现并解决问题

::warning
目前先写这么多,后续会继续完善这个文档.
::
