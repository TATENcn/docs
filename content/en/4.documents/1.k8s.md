---
title: kubernetes 技术说明
description: TATEN 服务器集群与容器编排、负载均衡、自动扩缩容等功能的底层技术细节和使用说明。
---

## 一、部署流程

### 1.创建 master 节点

首先，我们需要将一台云服务器作为我们的master节点，安装 kubernetes 组件并初始化集群。
配置 Kubernetes 单机环境

我们登录到准备加入 kubernetes 集群的机器，先设置主机名：

```bash [Terminal]
hostnamectl set-hostname k8s-master
```

然后关闭 swap 分区，这是因为 Kubernetes 要求关闭 swap 以确保性能和稳定性（详情可见 [Swap Off - why is it necessary?](https://discuss.kubernetes.io/t/swap-off-why-is-it-necessary/6879)）：

```bash [Terminal]
sudo swapoff -a
sudo sed -i '/ swap / s/^/#/' /etc/fstab
```

查看当前 swap 状态（没有输出 swap 大小信息表示 swap 已关闭）：

```bash [Terminal]
swapon --show
# 或
free -h
```

设置内核模块的加载（可参考 [Understanding br_netfilter in the Kubernetes Context](https://architecture-evolution.blogspot.com/2024/07/understanding-brnetfilter-in-kubernetes.html)）：

```bash [Terminal]
# 系统启动时加载 overlay 和 br_netfilter 模块
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
# 立即加载模块
sudo modprobe overlay && sudo modprobe br_netfilter
```

查看内核加载的配置信息：

```bash [Terminal]
# 查看配置信息(应该全部为 1)
sysctl net.bridge.bridge-nf-call-iptables
sysctl net.ipv4.ip_forward
sysctl net.bridge.bridge-nf-call-ip6tables
```

安装容器运行时，这里我们采用 containerd：

```bash [Terminal]
sudo apt update && sudo apt install -y containerd
```

生成 containerd 的默认配置文件，并且设置和 kubernetes 相同的 cgroup 驱动：

```bash [Terminal]
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml
```

重启 containerd 服务，并设置开机自启：

```bash [Terminal]
sudo systemctl daemon-reload
sudo systemctl restart containerd
sudo systemctl enable --now containerd
```

安装kubernetes相关组件（kuberadm、kubelet、kubectl）：
可以参考[安装 kubeadm](https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubeadm/install-kubeadm/)

```bash [Terminal]
sudo apt-get install -y apt-transport-https ca-certificates curl gnupg
# 如果 `/etc/apt/keyrings` 目录不存在，则应在 curl 命令之前创建它，请阅读下面的注释。
# sudo mkdir -p -m 755 /etc/apt/keyrings
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.35/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
sudo chmod 644 /etc/apt/keyrings/kubernetes-apt-keyring.gpg

# 添加合适的 Kubernetes `apt` 仓库。如果你想用 v1.35 之外的 Kubernetes 版本， 请将下面命令中的 v1.35 替换为所需的次要版本：
# 这会覆盖 /etc/apt/sources.list.d/kubernetes.list 中的所有现存配置
echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.35/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo chmod 644 /etc/apt/sources.list.d/kubernetes.list   # 有助于让诸如 command-not-found 等工具正常工作

#更新 `apt` 包索引，安装 kubelet、kubeadm 和 kubectl，并锁定其版本：
sudo apt update
sudo apt-get install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl
```

::tip
到这里，上面的内容是 Kubernetes 集群中每个 node 都需要进行的环境配置工作，下面描述 master 节点的配置工作。
::

在 master 节点上，我们使用 kubeadm 工具来初始化集群（详情可以参考[使用 kubeadm 创建集群](https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/)）：

```bash [Terminal]
sudo kubeadm init \
    --pod-network-cidr=10.244.0.0/16 \
    --cri-socket=unix:///var/run/containerd/containerd.sock \
    --control-plane-endpoint=powercess.com:6443
# control-plane-endpoint指定了集群的外部LB的地址。
# 以上配置仅供参考，以实际情况为准。

# 使用国内轩辕镜像来加速 kubeadm 初始化过程：
# 这里的 xxx-k8s.xuanyuan.run 是轩辕镜像仓库的个人账户专属地址，实际使用时需替换为正确的地址。
sudo kubeadm init \
  --pod-network-cidr=10.244.0.0/16 \
  --cri-socket=unix:///var/run/containerd/containerd.sock \
  --image-repository=xxx-k8s.xuanyuan.run \
  --control-plane-endpoint=powercess.com:6443
```

配置 Kubectl 工具，使 master 节点可以访问集群中的资源。

```bash [Terminal]
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

::warning
最好以非 root 用户来执行以上命令，确保 `$HOME/.kube/config` 文件的权限正确设置。
::

到此，master 节点的配置工作已经完成。

### 2. 为集群安装 CNI 插件

Kubernetes 集群需要一个 [CNI 插件](https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#pod-network)来管理 pod 网络。我们这里使用 Cilium 作为 CNI 插件：
相对来说，cilium的性能和功能都比较强大，适合生产环境使用。

cilium 的安装方式很多，我们这里采用方便快捷的 helm 包管理工具来安装 cilium：
你需要在可以访问集群的机器上安装 helm 工具，安装方式可以参考[helm 官方文档](https://helm.sh/zh/docs/intro/install/)。
因为master节点是ubuntu操作系统，所以我们执行如下的命令

```bash [Terminal]
sudo apt-get install curl gpg apt-transport-https --yes
curl -fsSL https://packages.buildkite.com/helm-linux/helm-debian/gpgkey | gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg > /dev/null
echo "deb [signed-by=/usr/share/keyrings/helm.gpg] https://packages.buildkite.com/helm-linux/helm-debian/any/ any main" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list
sudo apt-get update
sudo apt-get install helm
```

安装好之后，我们就可以使用 helm 命令来安装 cilium 了（安装方式可以参考 [cilium 文档](https://docs.cilium.io/en/stable/installation/k8s-install-helm/)）：

```bash [Terminal]
# 采用 OCI 容器注册表的方式安装 cilium
helm install cilium oci://quay.io/cilium/charts/cilium --version 1.19.1 --namespace kube-system

# 如果是国内安装，我们可以使用轩辕镜像来加速 cilium 的安装过程：
helm install cilium oci://xxx-k8s.xuanyuan.run/cilium/charts/cilium --version 1.19.1 --namespace kube-system
```

安装完成后，我们还需要安装 cilium-cli 工具，来验证 cilium 是否安装成功：

```bash [Terminal]
cilium status --verbose
```

正确安装并运行的 cilium 应该会显示类似如下的输出：

```bash
[cn059@cn059Workstation ~]$ cilium-cli status --verbose
    /¯¯\
 /¯¯\__/¯¯\    Cilium:             OK
 \__/¯¯\__/    Operator:           OK
 /¯¯\__/¯¯\    Envoy DaemonSet:    OK
 \__/¯¯\__/    Hubble Relay:       disabled
    \__/       ClusterMesh:        disabled

DaemonSet              cilium                   Desired: 1, Ready: 1/1, Available: 1/1
DaemonSet              cilium-envoy             Desired: 1, Ready: 1/1, Available: 1/1
Deployment             cilium-operator          Desired: 1, Ready: 1/1, Available: 1/1
Containers:            cilium                   Running: 1
                       cilium-envoy             Running: 1
                       cilium-operator          Running: 1
                       clustermesh-apiserver
                       hubble-relay
Cluster Pods:          2/2 managed by Cilium
Helm chart version:    1.19.1
Image versions         cilium             quay.io/cilium/cilium:v1.19.1@sha256:41f1f74a0000de8656f1de4088ea00c8f2d49d6edea579034c73c5fd5fe01792: 1
                       cilium-envoy       quay.io/cilium/cilium-envoy:v1.35.9-1770979049-232ed4a26881e4ab4f766f251f258ed424fff663@sha256:8188114a2768b5f49d6ce58e168b20d765e0fbc64eee0d83241aa2b150ccd788: 1
                       cilium-operator    quay.io/cilium/operator-generic:v1.19.1@sha256:e7278d763e448bf6c184b0682cf98cdca078d58a27e1b2f3c906792670aa211a: 1
```

或者是直接查看 cilium 的 pod 是否正常运行：

```bash [Terminal]
kubectl get pods -n kube-system
```

运行结果如下：

```bash
[cn059@cn059Workstation ~]$ kubectl get pods -n kube-system
NAME                              READY   STATUS    RESTARTS   AGE
cilium-dgj52                      1/1     Running   0          65m
cilium-envoy-7w745                1/1     Running   0          65m
cilium-operator-d5845585d-m2jd4   1/1     Running   0          65m
coredns-57b45d88c5-jzjlk          1/1     Running   0          109m
coredns-57b45d88c5-v8578          1/1     Running   0          109m
etcd-taten                        1/1     Running   0          109m
kube-apiserver-taten              1/1     Running   0          109m
kube-controller-manager-taten     1/1     Running   0          109m
kube-proxy-g6fhq                  1/1     Running   0          109m
kube-scheduler-taten              1/1     Running   0          109m
```

这个时候我们可以执行 `kubectl get nodes` 来查看集群中的节点状态：

```bash [Terminal]
kubectl get nodes
```

可以发现，集群中的 master 节点已经处于 Ready 状态了：

```bash
[cn059@cn059Workstation ~]$ kubectl get nodes
NAME    STATUS   ROLES           AGE    VERSION
taten   Ready    control-plane   111m   v1.35.2
```

### 3. worker 节点退出集群（可选）

如果我们需要把一个其他集群的 worker 节点加入到当前集群中，那么我们需要先从原集群移除节点，再加入新的集群

::steps{level="4"}
#### 标记节点为不可调度（cordon）

```bash [Terminal]
kubectl cordon <node-name>
```

效果如下：

```bash
~> kubectl cordon usc-k8s-worker
node/usc-k8s-worker cordoned
~> kubectl get nodes
NAME              STATUS                     ROLES           AGE     VERSION
hd-server         Ready                      worker          5d20h   v1.35.1
hka-k8s-server    Ready                      control-plane   7d14h   v1.35.1
lhka-k8s-worker   Ready                      worker          5d17h   v1.35.1
usa-k8s-worker    Ready                      storage         7d11h   v1.35.1
usb-k8s-worker    Ready                      worker          7d14h   v1.35.1
usc-k8s-worker    Ready,SchedulingDisabled   worker          7d14h   v1.35.1
usd-k8s-master    Ready                      control-plane   7d      v1.35.1
use-k8s-worker    Ready                      database        6d4h    v1.35.1
```

#### 驱逐节点上的 Pod（drain）

```bash [Terminal]
kubectl drain <node-name> --ignore-daemonsets --delete-emptydir-data
```
在执行这里的命令时，我的 node 出现了错误：

```bash
> kubectl drain usc-k8s-worker --ignore-daemonsets --delete-emptydir-data
node/usc-k8s-worker already cordoned
Warning: ignoring DaemonSet-managed Pods: kube-system/cilium-4x4fd, kube-system/cilium-envoy-kfrbp, monitoring/node-exporter-g6mrb
evicting pod napcat/napcat-5699966bdc-qvjjg
evicting pod default/gitea-postgresql-ha-pgpool-76c988848d-26xmg
evicting pod default/gitea-58487bcbc9-k5p69
evicting pod default/gitea-postgresql-ha-postgresql-1
evicting pod default/gitea-valkey-cluster-1
error when evicting pods/"gitea-valkey-cluster-1" -n "default" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
error when evicting pods/"gitea-postgresql-ha-postgresql-1" -n "default" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
error when evicting pods/"gitea-postgresql-ha-pgpool-76c988848d-26xmg" -n "default" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
```
这其实是因为这些 pod 都受到了 PDB（Pod Disruption Budget）的保护，无法被驱逐。我这里直接把 helm 安装的 gitea 删除掉即可：

```bash [Terminal]
~> helm list -n default
NAME                            NAMESPACE       REVISION        UPDATED                                         STATUS          CHART                                   APP VERSION
gitea                           default         1               2026-02-26 16:43:45.848560002 +0800 +0800       deployed        gitea-12.5.0                            1.25.4
nfs-subdir-external-provisioner default         1               2026-02-20 22:47:06.251937425 +0800 +0800       deployed        nfs-subdir-external-provisioner-1.0.0   4.0.18
~> helm uninstall gitea -n default
These resources were kept due to the resource policy:
[PersistentVolumeClaim] gitea-shared-storage

release "gitea" uninstalled
~> helm list -n default
NAME                            NAMESPACE       REVISION        UPDATED                                         STATUS          CHART                                   APP VERSION
nfs-subdir-external-provisioner default         1               2026-02-20 22:47:06.251937425 +0800 +0800       deployed        nfs-subdir-external-provisioner-1.0.0   4.0.18
```

随后就可以正确地执行 `kubectl drain` 命令了：

```bash [Terminal]
~> kubectl drain usc-k8s-worker --ignore-daemonsets --delete-emptydir-data
node/usc-k8s-worker already cordoned
Warning: ignoring DaemonSet-managed Pods: kube-system/cilium-4x4fd, kube-system/cilium-envoy-kfrbp, monitoring/node-exporter-g6mrb
node/usc-k8s-worker drained
```

#### 从集群中移除节点

```bash [Terminal]
kubectl delete node <node-name>
```

#### 在节点上清理 Kubernetes 组件

```bash [Terminal]
# 停止 kubelet 和容器运行时
sudo systemctl stop kubelet
sudo systemctl stop containerd

# 清理网络配置
sudo rm -rf /etc/cni/net.d/
# 清空当前系统中所有 iptables 规则表中的规则链
sudo iptables -F && sudo iptables -t nat -F && sudo iptables -t mangle -F && sudo iptables -t raw -F

# 删除 kubelet 配置和证书
sudo rm -rf /var/lib/kubelet/
sudo rm -rf /etc/kubernetes/
sudo rm -rf /var/lib/etcd/  # 如果该节点曾是 control-plane

# 重置 kubeadm（如果使用 kubeadm 安装）
sudo kubeadm reset --force
```

在清理 `/etc/kubernetes/` 时，可能会出现类似错误：
```bash
root@usc-k8s-worker:~# sudo rm -rf /var/lib/kubelet/
rm: cannot remove '/var/lib/kubelet/pods/0db3f606-6787-4cd4-b475-b730310fbd4e/volumes/kubernetes.io~projected/clustermesh-secrets': Device or resource busy
rm: cannot remove '/var/lib/kubelet/pods/0db3f606-6787-4cd4-b475-b730310fbd4e/volumes/kubernetes.io~projected/hubble-tls': Device or resource busy
rm: cannot remove '/var/lib/kubelet/pods/0db3f606-6787-4cd4-b475-b730310fbd4e/volumes/kubernetes.io~projected/kube-api-access-bgj6c': Device or resource busy
rm: cannot remove '/var/lib/kubelet/pods/48f6bafb-964e-4cec-9ba5-83d2cb1d935d/volumes/kubernetes.io~projected/kube-api-access-lrn59': Device or resource busy
rm: cannot remove '/var/lib/kubelet/pods/c5cead71-a6d8-4637-a94e-7eb5c6bf1819/volumes/kubernetes.io~projected/kube-api-access-zrvks': Device or resource busy
root@usc-k8s-worker:~#
find /var/lib/kubelet/pods -type d -exec mountpoint -q {} \; -print
/var/lib/kubelet/pods/0db3f606-6787-4cd4-b475-b730310fbd4e/volumes/kubernetes.io~projected/clustermesh-secrets
/var/lib/kubelet/pods/0db3f606-6787-4cd4-b475-b730310fbd4e/volumes/kubernetes.io~projected/hubble-tls
/var/lib/kubelet/pods/0db3f606-6787-4cd4-b475-b730310fbd4e/volumes/kubernetes.io~projected/kube-api-access-bgj6c
/var/lib/kubelet/pods/48f6bafb-964e-4cec-9ba5-83d2cb1d935d/volumes/kubernetes.io~projected/kube-api-access-lrn59
/var/lib/kubelet/pods/c5cead71-a6d8-4637-a94e-7eb5c6bf1819/volumes/kubernetes.io~projected/kube-api-access-zrvks
```
这是因为这些目录被 `内核挂载（mount）` 了，我们需要先卸载（umount）这些目录才能删除：

```bash [Terminal]
for m in $(mount | grep "/var/lib/kubelet/pods" | awk '{print $3}'); do
  sudo umount "$m"
done
```
::

### 4. worker 节点加入集群

在 master 节点上执行 `kubeadm token create --print-join-command` 来获取 worker 节点加入集群的命令：

```bash
# 示例输出
kubeadm join powercess.com:6443 --token qy69uc.ti9d3js6xmx7830f --discovery-token-ca-cert-hash sha256:a01a138f1557b94de36eigagh9iowqe26047b0bddeac7c7be86ac
```

保留好这个命令，我们需要初始化 worker 节点的环境：
实际上，worker 的环境配置和 master 节点的配置一样，只是不需要执行 `kubeadm init` 以后的命令来初始化集群。
这里我们不再赘述，直接在worker设备执行上面获取的 `kubeadm join` 命令来加入 worker 节点：

启动 kubelet 和 containerd 服务：

```bash [Terminal]
sudo systemctl start kubelet
sudo systemctl start containerd
```
